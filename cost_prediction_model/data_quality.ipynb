{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca11494d-3da3-4a9a-addb-6622b00c108a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyyaml -i https://artifactory.healthpartners.com/artifactory/api/pypi/python-hosted-remote/simple\n",
    "import warnings\n",
    "import pyspark.sql\n",
    "import Databricks.SharedModules.notification_utils as notif\n",
    "import yaml\n",
    "from pyspark.sql.functions import col, when\n",
    "from typing import Any, Optional, List\n",
    "from datetime import date, datetime, timedelta\n",
    "from Databricks.SharedModules.data_quality_exceptions import *\n",
    "from Databricks.SharedModules.delta_table_utils import delta_table_record_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6461a3ec-ba6f-463e-b614-a1147643ede5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataQualityWarning(Warning):\n",
    "    \"\"\" \n",
    "        Custom class for DataQualityWarnings. This allows notification logic \n",
    "        to send messages when there's a warning related to data quality but \n",
    "        not when there's a deprecation warning or something like that.\n",
    "    \"\"\"\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba131f7-f4b9-4bf5-aa8c-1a792abf8a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def alert(warning, full_table_name, source_environment, target_environment):\n",
    "    \"\"\" \n",
    "        Sends a Teams alert to the relevant webhook when called.\n",
    "    \"\"\"\n",
    "    if source_environment == 'prd' and target_environment == 'prd':\n",
    "        subtitle = 'PRODUCTION'\n",
    "        webhook = 'https://healthpartnersconnect.webhook.office.com/webhookb2/8e6d2cfc-f3ff-4678-ab5e-f98806ce95ef@9539230a-5213-4542-9ca6-b0ec58c41a4d/IncomingWebhook/c54f59cb1b5244d7a50a0ba73798933e/f8558c21-1856-4b0f-b1f3-e0e09dbd7eda'\n",
    "    else:\n",
    "        subtitle = 'Non-Prod'\n",
    "        webhook = 'https://healthpartnersconnect.webhook.office.com/webhookb2/8e6d2cfc-f3ff-4678-ab5e-f98806ce95ef@9539230a-5213-4542-9ca6-b0ec58c41a4d/IncomingWebhook/888eb648756b446f98ec5061d8b90168/f8558c21-1856-4b0f-b1f3-e0e09dbd7eda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8518cddb-02f2-434a-bdb8-35a94c14b847",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def source_dataframe_populated(extract_sdf: pyspark.sql.DataFrame,\n",
    "                               full_table_name: str,\n",
    "                                source_environment: str,\n",
    "                                target_environment: str,\n",
    "                                warning_num_obs: Optional[int] = None,\n",
    "                                error_num_obs: Optional[int] = 1) -> int:\n",
    "    \"\"\" Sends a warning if the number of records in the source pyspark dataframe\n",
    "        is less than warning_num_obs. Raises an error if less than error_num_obs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        extract_sdf: pyspark.sql.Dataframe\n",
    "                The source data extract as a pyspark dataframe\n",
    "        warning_num_obs: int, default None\n",
    "                A warning is logged if the number of records in source_sdf is \n",
    "                less than warning_num_obs\n",
    "        error_num_obs: int, default 1\n",
    "                An error is raised if the number of records in source_sdf is\n",
    "                less than error_num_obs\n",
    "        \n",
    "        Raises\n",
    "        ----------\n",
    "        SourceDataframeNotPopulatedError\n",
    "                If fewer than error_num_obs records in source_sdf.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        extract_num_obs: int\n",
    "                Number of records in the source_sdf. Can use if trying to avoid\n",
    "                counting number of records multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    extract_num_obs = extract_sdf.count()\n",
    "\n",
    "    if extract_num_obs < error_num_obs:\n",
    "        print(\"***ERROR***\")\n",
    "        alert(f\"source_dataframe_populated failure: number of source records is less than the error_threshold ({error_num_obs})\", full_table_name, source_environment, target_environment)\n",
    "        raise SourceDataframeNotPopulatedError(error_num_obs=error_num_obs,\n",
    "                                               dataframe_num_obs=extract_num_obs)\n",
    "    elif warning_num_obs is not None and (extract_num_obs < warning_num_obs):\n",
    "        print(\"***WARNING***\")\n",
    "        warnings.warn(f'Only {extract_num_obs} records were detected in the source dataframe. This is less than the warning_threshold ({warning_num_obs}).',\n",
    "                      DataQualityWarning)\n",
    "    else:\n",
    "        print(\"source_dataframe_populated SUCCESS\")\n",
    "\n",
    "    return extract_num_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f2356d-ef13-4cdc-8fed-788ecb7be8d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_completeness(catalog: str, schema: str, table: str,\n",
    "                      to_load_sdf: pyspark.sql.DataFrame,\n",
    "                      spark: pyspark.sql.SparkSession,\n",
    "                      source_environment: str,\n",
    "                      target_environment: str,\n",
    "                      warning_threshold: Optional[float] = 0.75,\n",
    "                      error_threshold: Optional[float] = 0.05) -> None:\n",
    "    \"\"\" Sends errors or warnings if number of records in data to be loaded is less \n",
    "        than a given percentage of the existing records in the target table. This check\n",
    "        makes sense only for upsert or trunc and load strategy, not insert-only.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        catalog: str\n",
    "                Name of the catalog where the target table is stored (e.g. 'cleansed')\n",
    "        schema: str\n",
    "                Name of the target table's schema (e.g. 'care_group')\n",
    "        table: str\n",
    "                Name of the target table (e.g. 'emr_patient_identity')\n",
    "        to_load_sdf: pyspark.sql.DataFrame\n",
    "                The pyspark dataframe we're planning to load into the target table\n",
    "        spark: pyspark.sql.SparkSession\n",
    "                The active spark session. In python notebooks, this variable is \n",
    "                automatically created and named spark.\n",
    "        warning_threshold: Optional, float between 0 and 1\n",
    "                Send a warning if fewer than threshold * existing number of records\n",
    "                found in data to be loaded. Set to None to not send warnings.\n",
    "        error_threshold: Optional, float between 0 and 1\n",
    "                Raise an error if fewer than threshold * existing number of records\n",
    "                found in data to be loaded. Set to None to not raise errors.\n",
    "    \"\"\"\n",
    "    target_table_count = delta_table_record_count(catalog=catalog, schema=schema,\n",
    "                                            table=table, spark=spark)\n",
    "    load_sdf_count = to_load_sdf.count()\n",
    "\n",
    "    if error_threshold is not None and (load_sdf_count < error_threshold * target_table_count):\n",
    "        print(\"***ERROR***\")        \n",
    "        alert(f\"data_completeness failure: number of source records is less than {error_threshold * 100}% of records already in {full_table_name})\", full_table_name, source_environment, target_environment)\n",
    "        raise DataCompletenessError(load_sdf_count=load_sdf_count, target_table_count=target_table_count, table=table)\n",
    "    elif warning_threshold is not None and (load_sdf_count < warning_threshold * target_table_count):\n",
    "        print(\"***WARNING***\")\n",
    "        warnings.warn('Only {load_sdf_count} records to be added to {table}, but {target_table_count} records currently in table. Check for possible data quality issue on load.'.format(\n",
    "            load_sdf_count=load_sdf_count,\n",
    "            table=table,\n",
    "            target_table_count=target_table_count\n",
    "            ),\n",
    "            DataQualityWarning)\n",
    "    else:\n",
    "        print(\"data_completeness SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbbdad85-e4e0-479d-8e92-a6caa6fabc59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def key_validation(sdf: pyspark.sql.DataFrame,\n",
    "                   full_table_name: str,\n",
    "                   source_environment: str, \n",
    "                   target_environment: str,\n",
    "                    key_columns: List[str]) -> None:\n",
    "    \"\"\" Verifies that the pyspark dataframe is not duplicated by the key_columns.\n",
    "        Use to validate the primary and natural keys.\n",
    "\n",
    "        Note: This can also be checked with great expectations and if you have a \n",
    "            great expectations test suite in your project, that is the recommended \n",
    "            route.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sdf: pyspark.sql.DataFrame\n",
    "                The pyspark dataframe to check for bad dates\n",
    "        key_columns: List[str]\n",
    "                A list of the column names by which the data should be unique\n",
    "        \n",
    "        Raises\n",
    "        ----------\n",
    "        KeyViolationError\n",
    "                If the pyspark dataframe is not unique by the key_columns,\n",
    "                then a KeyViolationError is raised.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "    \"\"\"\n",
    "    if sdf.count() != sdf.select(key_columns).distinct().count():\n",
    "        alert(f\"key_validation failure: duplicate row(s) exist across primary key columns\", full_table_name, source_environment, target_environment)\n",
    "        raise KeyViolationError(key_columns)\n",
    "    else:\n",
    "        print(\"key_validation SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9914d5-e3e7-44cb-bfa9-c8e8a9176fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def date_range_validation(sdf: pyspark.sql.DataFrame,\n",
    "                          date_columns: List[str],\n",
    "                          bad_dates_allowed: int,\n",
    "                          min_allowable_dt: datetime.date = (datetime.now() - timedelta(days=365*200)).date(),\n",
    "                          max_allowable_dt: datetime.date = datetime.now().date()):\n",
    "                        #   error_max_replacements: Optional[int] = None) -> pyspark.sql.DataFrame:\n",
    "    \"\"\" If a date column is outside of the range defined by min_allowable_dt\n",
    "        and max_allowable_dt and not equal to 9999-01-01, then the value is replaced by\n",
    "        9999-01-01 and a warning is issued.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sdf: pyspark.sql.DataFrame\n",
    "                The pyspark dataframe to check for bad dates\n",
    "        date_columns: List[str]\n",
    "                A list of the column names to check for dates outside the range\n",
    "        min_allowable_dt: datetime.date, default 200 years ago (give or take a day)\n",
    "                Dates less than min_allowable_dt will be replaced with a warning\n",
    "        max_allowable_dt: datetime.date, default today\n",
    "                Dates greater than max_allowable_dt will be replaced with a warning\n",
    "        error_max_replacements: Optional[int], default None\n",
    "                If more than error_max_replacements occur in a single column,\n",
    "                an exception is raised. This field is intended to catch problems\n",
    "                such as an entire column of bad dates for which we actually want\n",
    "                to kill the job rather than replace the values.\n",
    "        \n",
    "        Raises\n",
    "        ----------\n",
    "        DateRangeViolationError\n",
    "                If more than error_max_replacements records have values that are\n",
    "                outside the allowable date range, a DataRangeViolationError is \n",
    "                raised.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        sdf: pyspark.sql.DataFrame\n",
    "                The dataframe following bad date replacement\n",
    "    \"\"\"\n",
    "    date_replacement = datetime.strptime('9999-01-01', '%Y-%m-%d').date()\n",
    "    print()\n",
    "    # Replace out-of-range values with '9999-01-01'\n",
    "    for dc in date_columns:\n",
    "        # bad_date_count is # of dates either not in allowed range OR not equal to replacement date \n",
    "        bad_date_count = sdf.filter((col(dc) < min_allowable_dt) | (col(dc) > max_allowable_dt)).count()\n",
    "        print(f\" current column: {dc}\")\n",
    "        print(f\" bad_date_count: {bad_date_count}\")\n",
    "\n",
    "        # if # bad dates is greater than max replacement value, raise error\n",
    "        if bad_date_count > bad_dates_allowed:\n",
    "            raise DateRangeValidationError(column_name=dc, bad_date_count=bad_date_count,\n",
    "                                            error_max_replacements=bad_dates_allowed)\n",
    "        elif bad_date_count > 0:\n",
    "            print(\"***WARNING***\")\n",
    "            warnings.warn('{bad_date_count} invalid dates were detected in the {dc} column. This is less than the allowed number {bad_dates_allowed} so these dates were ignored'.format(\n",
    "                    bad_date_count=bad_date_count,\n",
    "                    dc=dc,\n",
    "                    bad_dates_allowed=bad_dates_allowed\n",
    "                    ),\n",
    "                    DataQualityWarning)\n",
    "    print(\"date_range_validation SUCCESS\")\n",
    "        # if error_max_replacements is not None and bad_date_count > error_max_replacements:\n",
    "        #     raise DateRangeValidationError(column_name=dc, bad_date_count=bad_date_count,\n",
    "        #                                     error_max_replacements=error_max_replacements)\n",
    "        # # if # bad dates is NOT greater than max replacement value and is not zero, replace bad dates with replacement & send warning with # of dates replaced\n",
    "        # elif bad_date_count > 0:\n",
    "        #     sdf = sdf.withColumn(dc, when((col(dc) != date_replacement) &\n",
    "        #                                     (col(dc) < min_allowable_dt) | (col(dc) > max_allowable_dt),\n",
    "        #                                 date_replacement).otherwise(col(dc)))\n",
    "        #     warnings.warn('{bad_date_count} values were replaced with {replacement} in the {dc} column.'.format(\n",
    "        #             bad_date_count=bad_date_count,\n",
    "        #             replacement=date_replacement,\n",
    "        #             dc=dc\n",
    "        #             ),\n",
    "        #             DataQualityWarning)\n",
    "    return sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b6e65f-daa3-4543-b7b6-d8ab00bd9c0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def null_value_replacement(sdf: pyspark.sql.DataFrame,\n",
    "#                         non_null_columns: List[str],\n",
    "#                         error_max_replacements: Optional[int] = None) -> pyspark.sql.DataFrame:\n",
    "#         \"\"\" If a null value is detected in a column required to be non-null,\n",
    "#                 a warning is sent and the value is replaced. Note that this data\n",
    "#                 quality check is intended to serve as a backup measure for unexpected\n",
    "#                 null data coming in from the source and should not be a first approach\n",
    "#                 to null value replacement. Additionally, these checks are not a\n",
    "#                 replacement for appropriately identifying non-null columns in your DDL\n",
    "#                 statement and consequently in your table's metadata.\n",
    "\n",
    "#                 Numeric ID 9999999999 (ten 9's), Dates and Times:  9999-01-01,\n",
    "#                 Strings: 'Not Available',  Y/N:  'N'\n",
    "\n",
    "#                 Data types are derived from column names and types.\n",
    "\n",
    "#                 Parameters\n",
    "#                 ----------\n",
    "#                 sdf: pyspark.sql.DataFrame\n",
    "#                         The pyspark dataframe to check for nulls\n",
    "#                 non_null_columns: List[str]\n",
    "#                         A list of the column names to check for null values\n",
    "#                 error_max_replacements: Optional[int], default None\n",
    "#                         If more than error_max_replacements occur in a single column,\n",
    "#                         an error is raised.\n",
    "                \n",
    "#                 Raises\n",
    "#                 ----------\n",
    "#                 NullValueReplacementError\n",
    "#                         If more than error_max_replacements records have updates to a\n",
    "#                         single column OR there are any nulls in a numeric column without\n",
    "#                         suffix _id then a NullValueReplacementError is raised.\n",
    "                \n",
    "#                 Returns\n",
    "#                 ----------\n",
    "#                 sdf: pyspark.sql.DataFrame\n",
    "#                         The dataframe following null value replacement\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Define replacement values for different data types\n",
    "#         id_replacement = 9999999999\n",
    "#         timestamp_replacement = datetime.strptime('9999-01-01', '%Y-%m-%d')\n",
    "#         date_replacement = timestamp_replacement.date()\n",
    "#         string_replacement = 'Not Available'\n",
    "#         yn_replacement = 'N'\n",
    "\n",
    "\n",
    "#         # Replace null values with appropriate replacement values\n",
    "#         for nnc in non_null_columns:\n",
    "#                 null_count = sdf.filter(col(nnc).isNull()).count()\n",
    "\n",
    "#                 if error_max_replacements is not None and null_count > error_max_replacements:\n",
    "#                 raise NullValueReplacementError(column_name=nnc, null_value_count=null_count,\n",
    "#                                                 threshold_error=True, error_max_replacements=error_max_replacements)\n",
    "#                 elif null_count > 0:\n",
    "#                 if nnc[-3:] == '_id' and sdf.schema[nnc].dataType.simpleString() in ['int', 'bigint']:\n",
    "#                         sdf = sdf.withColumn(nnc, when(col(nnc).isNull(), id_replacement).otherwise(col(nnc)))\n",
    "#                         warnings.warn('{null_count} values were replaced with {replacement} in the {nnc} column.'.format(\n",
    "#                         null_count=null_count,\n",
    "#                         replacement=id_replacement,\n",
    "#                         nnc=nnc\n",
    "#                         ),\n",
    "#                         DataQualityWarning)\n",
    "#                 elif nnc[-3:] == '_yn':\n",
    "#                         sdf = sdf.withColumn(nnc, when(col(nnc).isNull(), yn_replacement).otherwise(col(nnc)))\n",
    "#                         warnings.warn('{null_count} values were replaced with {replacement} in the {nnc} column.'.format(\n",
    "#                         null_count=null_count,\n",
    "#                         replacement=yn_replacement,\n",
    "#                         nnc=nnc\n",
    "#                         ),\n",
    "#                         DataQualityWarning)\n",
    "#                 elif sdf.schema[nnc].dataType.simpleString() == 'date':\n",
    "#                         sdf = sdf.withColumn(nnc, when(col(nnc).isNull(), date_replacement).otherwise(col(nnc)))\n",
    "#                         warnings.warn('{null_count} values were replaced with {replacement} in the {nnc} column.'.format(\n",
    "#                         null_count=null_count,\n",
    "#                         replacement=date_replacement,\n",
    "#                         nnc=nnc\n",
    "#                         ),\n",
    "#                         DataQualityWarning)\n",
    "#                 elif sdf.schema[nnc].dataType.simpleString() == 'timestamp':\n",
    "#                         sdf = sdf.withColumn(nnc, when(col(nnc).isNull(), timestamp_replacement).otherwise(col(nnc)))\n",
    "#                         warnings.warn('{null_count} values were replaced with {replacement} in the {nnc} column.'.format(\n",
    "#                         null_count=null_count,\n",
    "#                         replacement=timestamp_replacement,\n",
    "#                         nnc=nnc\n",
    "#                         ),\n",
    "#                         DataQualityWarning)\n",
    "#                 elif sdf.schema[nnc].dataType.simpleString() == 'string':\n",
    "#                         sdf = sdf.withColumn(nnc, when(col(nnc).isNull(), string_replacement).otherwise(col(nnc)))\n",
    "#                         warnings.warn('{null_count} values were replaced with {replacement} in the {nnc} column.'.format(\n",
    "#                         null_count=null_count,\n",
    "#                         replacement=string_replacement,\n",
    "#                         nnc=nnc\n",
    "#                         ),\n",
    "#                         DataQualityWarning)\n",
    "#                 else:\n",
    "#                         raise NullValueReplacementError(column_name=nnc, null_value_count=null_count, threshold_error=False)\n",
    "\n",
    "#         return sdf\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_quality",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
