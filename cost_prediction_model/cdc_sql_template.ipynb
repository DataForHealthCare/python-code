{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8fe8ce-18c6-4ee4-b5f0-1f1ab544c72d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Change data capture template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02599a13-4313-49a0-a81b-76ab8c628bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Usage:\n",
    "- This notebook contains generic code that will run using the changes you have made to index.yaml and extract_config.yaml.\n",
    "  - *DO NOT* make changes to this notebook!\n",
    "- refer to [README](https://adb-1501466301957626.6.azuredatabricks.net/?o=1501466301957626#notebook/2271376384770036/command/2271376384770037) for instructions.\n",
    "- refer to [writing_messy_code](https://confluence.healthpartners.com/confluence/pages/viewpage.action?pageId=297468803) if you are unfamilliar with using config and yaml files in databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d480fbb-8a1d-42bd-a943-fc0e932c728b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15dab96e-f3b2-4232-a85c-8ddf83e3e87e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Installs & imports\n",
    "%pip install --trusted-host artifactory.healthpartners.com pyyaml -i https://artifactory.healthpartners.com/artifactory/api/pypi/python-hosted-remote/simple\n",
    "%pip install --trusted-host artifactory.healthpartners.com great-expectations -i https://artifactory.healthpartners.com/artifactory/api/pypi/python-hosted-remote/simple\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "import Databricks.SharedModules.notification_utils as notif\n",
    "import Databricks.SharedModules.delta_table_utils as dtutil\n",
    "from Databricks.SharedModules.general import get_config_item, get_catalog_suffix\n",
    "from Databricks.SharedModules.gx_tool import SimpleGXTool\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab23ffc-5b88-49c6-b2fa-25568e2125d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Environment setup\n",
    "- Set up the environment and catalog variables by checking the current system environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97c76ea-e026-4695-ba0d-893c66382216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# From the Schedule button in the top right, create a new workflow for your job. Set key = source_environment, value = prd and key = target_environment, value = dev.\n",
    "# Going forward, run your notebook using the workflow, rather than interactively. You'll need to click into the job to see the output, rather than looking here.\n",
    "# Source and target environments from the job task\n",
    "# Comment out to run in notebook\n",
    "source_environment = dbutils.widgets.get('source_environment')\n",
    "target_environment = dbutils.widgets.get('target_environment')\n",
    "\n",
    "catalog ='standardized{}'.format(get_catalog_suffix(target_environment))\n",
    "\n",
    "try:\n",
    "    test_warning = bool(dbutils.widgets.get('test_warning'))\n",
    "except:\n",
    "    test_warning = False\n",
    "\n",
    "try:\n",
    "    test_error = bool(dbutils.widgets.get('test_error'))\n",
    "except:\n",
    "    test_error = False\n",
    "\n",
    "print('######################## ENVIRONMENT SETUP')\n",
    "print('source_environment: ', source_environment)\n",
    "print('target_environment: ', target_environment)\n",
    "print(f'Catalog: {catalog}')\n",
    "print('test_warning: ', test_warning)\n",
    "print('test_error: ', test_error)\n",
    "\n",
    "# # Uncomment below and comment out above to run in notebook\n",
    "# \t# Parameter assignment from job task.\n",
    "# try:\n",
    "#     environment = dbutils.widgets.get(\"environment\")\n",
    "# except:\n",
    "#     environment = 'development'\n",
    "# # Determination of output location based on job task parameter.\n",
    "# if environment == 'production':\n",
    "#     catalog = 'standardized' \n",
    "# else:\n",
    "#     catalog = 'standardized_dev'\n",
    "# source_environment = 'dev'\n",
    "# target_environment = 'dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3284fdc-c77b-4e38-a095-5b63c655e03b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setup\n",
    "- instantiate a python variable containing the info from your index.yaml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c6a1f5-70e5-4d3d-91be-86daca8fd388",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('./config/index.yaml', 'r') as f:\n",
    "  index_yaml = yaml.safe_load(f)\n",
    "\n",
    "schema = index_yaml['schema']\n",
    "table = index_yaml['table']\n",
    "table_name = table['name']\n",
    "full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "print('schema: ', schema)\n",
    "print('table name: ', table_name)\n",
    "print('fully qualified table name: ', full_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7803f88a-8f57-4426-a3f8-5cfe663901d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data Quality Import\n",
    "- Run the data_quality notebook to gain access to it's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0d31839-a01f-45bb-ac1f-8d884cc965d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config/data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7cccc7a-1aac-49a9-8ef7-dbdab2de92cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def configure_great_expectations():\n",
    "    dbutils.notebook.run('./config/great_expectations_config',\n",
    "                        timeout_seconds=0,\n",
    "                        arguments={'target_environment': target_environment})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e800c900-892d-4415-8431-bb30edc6c8ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Extract source data\n",
    "- The function in the following cell creates a dataframe from the source data defined by the query in extract_config.yaml.\n",
    "- This function will raise an exception if the source dataframe is not populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e752a673-1441-4bba-b122-8814a246d549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract():\n",
    "    extract_qry = get_config_item('./config/extract_config.yaml', 'source1_extract')\n",
    "    my_table_name_sdf = spark.sql(extract_qry.format(suffix=get_catalog_suffix(source_environment)))\n",
    "    # check to see source dataframe isn't empty\n",
    "    source_dataframe_populated(my_table_name_sdf, full_table_name, source_environment, target_environment)\n",
    "\n",
    "    return my_table_name_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2642c3e9-d542-43b2-8501-8ab5c0f85f87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_primary_key_columns():    \n",
    "    primary_keys_list = []\n",
    "    for pk_num in range(1,len(table['primary_keys'])+1):\n",
    "        primary_key_column_name = [table['primary_keys']['pk_column_'+str(pk_num)]]\n",
    "        primary_keys_list += primary_key_column_name\n",
    "    return primary_keys_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6814000-bdfd-4079-8455-5eec44fb8757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validation\n",
    "- This function performs various data checks using functions in the data_quality.py file.\n",
    "- This function also uses gx_tool.py to invoke expectations and raise warmings or exceptions when the expectations are violated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5be6210-6837-49bd-a733-3267baa56df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate(my_table_name_sdf, catalog, schema, table):\n",
    "    # Run GX config file to import its functions\n",
    "    # configure_great_expectations()\n",
    "    # Verify record count is plausible\n",
    "    # - Sends warning if number of records loaded is less than warning_threshold\n",
    "    # - Raises error if number of records loaded is less than erorr_threshold\n",
    "    try:\n",
    "        x = spark.sql(f\"SELECT * from {full_table_name}\")\n",
    "    except:\n",
    "        data_completeness(catalog=catalog,\n",
    "                                schema=schema,\n",
    "                                table=table,\n",
    "                                to_load_sdf=my_table_name_sdf,\n",
    "                                spark=spark,\n",
    "                                source_environment = source_environment,\n",
    "                                target_environment = target_environment,\n",
    "                                warning_threshold= index_yaml['data_completeness_warning_threshold'],\n",
    "                                error_threshold= index_yaml['data_completeness_error_threshold'])\n",
    "\n",
    "    # Verify data is unique by the natural key\n",
    "    # - Verifies that the pyspark dataframe is not duplicated by the key_columns\n",
    "    primary_keys_list = get_primary_key_columns()\n",
    "    key_validation(sdf=my_table_name_sdf,\n",
    "                   full_table_name=full_table_name,\n",
    "                   source_environment=source_environment, \n",
    "                   target_environment=target_environment, \n",
    "                   key_columns=primary_keys_list)\n",
    "    \n",
    "    # Verify source dataframe is populated\n",
    "    # - Sends a warning if the number of records in the source pyspark dataframe is less than warning_num_obs. \n",
    "    # - Raises an error if less than error_num_obs.\n",
    "    source_dataframe_populated(extract_sdf= my_table_name_sdf,\n",
    "                               full_table_name= full_table_name, \n",
    "                               source_environment= source_environment, \n",
    "                               target_environment= target_environment,\n",
    "                                warning_num_obs= index_yaml['source_populated_warning_num_obs'],\n",
    "                                error_num_obs= index_yaml['source_populated_error_num_obs'])\n",
    "\n",
    "    # Verify columns with type date are in valid range\n",
    "    # - Sends warning if invalid dates present but not greater than the allowed number\n",
    "    # - Raises error if number of invalid dates exceeds the allowed value\n",
    "    date_cols = []\n",
    "    for col_name, col_data_type in my_table_name_sdf.dtypes:\n",
    "        if col_data_type == 'date':\n",
    "            date_cols.append(col_name)\n",
    "    print(f\"date colunns: {date_cols}\")\n",
    "    date_range_validation(sdf= my_table_name_sdf,\n",
    "                          date_columns= date_cols,\n",
    "                          bad_dates_allowed= index_yaml['bad_dates_allowed'],\n",
    "                          min_allowable_dt= (datetime.now() - timedelta(days=365*200)).date(),\n",
    "                        #   max_allowable_dt= (datetime.now() - timedelta(days=365)).date())\n",
    "                          max_allowable_dt= datetime.now().date())\n",
    "\n",
    "    # Create and run a great expectations checkpoint(s)\n",
    "    # gx_tool = SimpleGXTool(catalog=catalog, schema=schema, table=table)\n",
    "    # gx_tool.add_checkpoint(\n",
    "    #             expectation_suite_name='{catalog}_{schema}_{table}_expectation_suite_error'.format(\n",
    "    #                 catalog=catalog,\n",
    "    #                 schema=schema,\n",
    "    #                 table=table\n",
    "    #                 ),\n",
    "    #             dataframe=my_table_name_sdf)\n",
    "    # gx_tool.run_validation()\n",
    "\n",
    "    # gx_tool.add_checkpoint(\n",
    "    #             expectation_suite_name='{catalog}_{schema}_{table}_expectation_suite_warning'.format(\n",
    "    #                 catalog=catalog,\n",
    "    #                 schema=schema,\n",
    "    #                 table=table\n",
    "    #                 ),\n",
    "    #             dataframe=my_table_name_sdf)\n",
    "    # gx_tool.run_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167e24f6-07d3-46bf-882f-2b1b267016f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Add cell to create the schema if it doesnt exist\n",
    "# create the schema\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#         CREATE SCHEMA IF NOT EXISTS standardized{suffix}.YOURSCHEMANAME\n",
    "#         \"\"\".format(suffix=get_catalog_suffix(target_environment)))\n",
    "# spark.sql(\"\"\"\n",
    "#         CREATE SCHEMA IF NOT EXISTS standardized{suffix}.YOURSCHEMANAME\n",
    "#         \"\"\".format(suffix=get_catalog_suffix(target_environment)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6679068d-741a-4d51-b424-9195b505a710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create the initial table declared in index.yaml\n",
    "- This next cell creates the DDL for the new databricks delta table.\n",
    "- These initial tables will not include primary key or null constraints, these will be added in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7408ec3-d00b-465f-b76c-5e1a31b6b220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the table specified in index.yaml (no null constraints or primary keys yet)\n",
    "def create_initial_table():\n",
    "    # dynamically build string to declare new columns in query\n",
    "    column_string_list = []\n",
    "    column_list = list(table['columns'])\n",
    "    for column_num in range(0,len(table['columns'])):\n",
    "        column_name = column_list[column_num]\n",
    "        column = table['columns'][column_list[column_num]]\n",
    "        column_string = [f'''{column_name.upper()} {column['type']} COMMENT \"{column['comment']}\"''']\n",
    "        column_string_list += column_string\n",
    "    delim = ',\\n'\n",
    "    columns_string = delim.join(column_string_list)\n",
    "\n",
    "    # build full table query\n",
    "    create_table_query = f'''CREATE TABLE IF NOT EXISTS {full_table_name} (\n",
    "    {columns_string})\n",
    "        USING DELTA\n",
    "        TBLPROPERTIES (DELTA.EnableChangeDataFeed = true)\n",
    "    COMMENT \"{table['comment']}\"'''.format(catalog=catalog)\n",
    "    print(\"##### CREATE TABLE QUERY: #####\")\n",
    "    print(create_table_query)\n",
    "    print(\"\\n\")\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fea2e9-0dbf-4485-8039-4a66e70b022f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Add Null Constraints\n",
    "- The following cell will add null constraints to all columns included in the primary key and any additional columns specified as not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad35959f-52bd-417c-8a3e-76150924f8c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add not null constrains\n",
    "def add_null_constraints():\n",
    "    print(\"##### NULL CONSTRAINT STATEMENTS: ##### \")\n",
    "    # add null contraints for any columns that are part of the primary key\n",
    "    for pk_num in range(1,len(table['primary_keys'])+1):\n",
    "        primary_key_column = table['primary_keys']['pk_column_'+str(pk_num)]\n",
    "        add_null_prim_keys_query = f'''\n",
    "        ALTER TABLE {full_table_name}\n",
    "        ALTER COLUMN {primary_key_column.upper()} SET NOT NULL'''.format(catalog=catalog)\n",
    "        print(add_null_prim_keys_query)\n",
    "        spark.sql(add_null_prim_keys_query)\n",
    "    # add null constrains for any additional columns you want to specify as not allowing nulls\n",
    "    if table['additional_not_null_columns'] is not None:\n",
    "        for null_num in range(1,len(table['additional_not_null_columns'])+1):\n",
    "            null_column = table['additional_not_null_columns']['null_col'+str(null_num)]\n",
    "            add_additional_nulls_query = f'''\n",
    "            ALTER TABLE {full_table_name}\n",
    "            ALTER COLUMN {null_column.upper()} SET NOT NULL'''.format(catalog=catalog)\n",
    "            print(add_additional_nulls_query)\n",
    "            spark.sql(add_null_prim_keys_query)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a545652-b886-4321-8a22-3f42fc0aac51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Add Primary Key\n",
    "- The following cell will add primary key constraints for the columns specified in index.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "205a8655-eb46-4926-bc08-cbf8a59482c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add primary keys\n",
    "def add_primary_key_constraints():\n",
    "    print(\"##### PRIMARY KEY CONSTRAINT STATEMENTS: #####\")\n",
    "    primary_key_columns = []\n",
    "    for pk_num in range(1,len(table['primary_keys'])+1):\n",
    "            primary_key_column_name = [table['primary_keys']['pk_column_'+str(pk_num)]]\n",
    "            primary_key_columns = primary_key_columns + (primary_key_column_name)\n",
    "            pk_matching_string_to_add = f\"target.{primary_key_column_name} == source.{primary_key_column_name}\"\n",
    "    delim = ','\n",
    "    prim_keys_string = delim.join(primary_key_columns)\n",
    "    add_prim_keys_query = f'''\n",
    "            ALTER TABLE {full_table_name}\n",
    "            ADD CONSTRAINT {table_name.upper()}_PK PRIMARY KEY ({prim_keys_string})'''.format(catalog=catalog)\n",
    "    print(add_prim_keys_query)\n",
    "    print(\"\\n\")\n",
    "    spark.sql(add_prim_keys_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45cbea0-93ca-4e70-9414-9491e9e62fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80c63fd-ac16-4f94-a264-5ed5e798a269",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load index.yaml file\n",
    "- Extract the source query from your extract_config.yaml file via safe_load\n",
    "- This query will specify which data you want to load into your target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bb9431-fbbc-47b5-bc78-7d0af9aa5000",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start configuration by opening up yaml file\t\n",
    "def get_config_item(config_path, key):\n",
    "\twith open(config_path, 'r') as file:\n",
    "\t    config = yaml.safe_load(file)\n",
    "\treturn config[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e7cb93-eb72-49db-b643-2e60cb731816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Identify source column names\n",
    "The function in the following builds a string containing the names of the columns from your source data. These names are needed for the matching portion of the merge into statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243d76df-fd7d-4f78-8716-9200b4865f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get column names from source table and create formatted string\n",
    "def get_source_column_names():\n",
    "    source_query = get_config_item('./config/extract_config.yaml', 'source_query')\n",
    "    source_df = spark.sql(source_query)\n",
    "    source_column_list = []\n",
    "    for column in source_df.columns:\n",
    "        source_column_list += [f\"source_table.{column}\"]\n",
    "    delim = ','\n",
    "    source_columns_string = delim.join(source_column_list)\n",
    "    print(source_column_list)\n",
    "    print(source_columns_string)\n",
    "    return source_columns_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99f3bbb-dddd-4481-80ea-c06b52e4872e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Merge Condition (Primary Key Matching)\n",
    "The function in the following cell builds a string containing the merge condition that will be used to match the primary keys of the source and target tables in the merge statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2308c883-7b2e-472f-87a4-c54cb2dfe377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_merge_condition_string(table):\n",
    "    primary_keys_matchings_list = []\n",
    "    for pk_num in range(1,len(table['primary_keys'])+1):\n",
    "        primary_key_column_name = table['primary_keys']['pk_column_'+str(pk_num)]\n",
    "        primary_keys_matchings_list = primary_keys_matchings_list + [f\"source_table.{primary_key_column_name} = target.{primary_key_column_name}\"]\n",
    "    delim = ' AND '\n",
    "    merge_condition = delim.join(primary_keys_matchings_list)\n",
    "    return merge_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df37d5b3-963d-468d-b44c-8ed7972ea148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Column Matching Condition\n",
    "- The function in the following cell builds a string containing the column matching condition that will be used to load date into the new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1faf1e26-51e3-4d0f-8982-7ba22ed31a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_column_matchings_string(table):\n",
    "    column_matchings_list = []\n",
    "    columns_list = list(table['columns'])\n",
    "    for column_num in range(0,len(columns_list)):\n",
    "        column_name = columns_list[column_num]\n",
    "        column_matchings_list = column_matchings_list + [f\"{column_name} = target.{column_name}\"]\n",
    "    delim = ','\n",
    "    column_matchings_string = delim.join(column_matchings_list)\n",
    "    return column_matchings_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee759614-7a2e-4cd9-89a2-f9921b8bd107",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Identify Target Columns\n",
    "- The function in the following cell builds a string containing the names of the columns in the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7395d3c7-c87c-40fa-b888-55f40578d00e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_target_columns_string(table):\n",
    "    columns_list = list(table['columns'])\n",
    "    delim = ','\n",
    "    return delim.join(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2d6aec-0daf-434a-9a9e-0778aeeb37cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Merge Data Into Table\n",
    "- The following cell will load the data specified in the extract_config.yaml query into the new table.\n",
    "- This cell works by creating strings representing portions of the merge into statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb26197-e8fe-4e86-a2ad-f5ea74e6384a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#merge source data into each new table\n",
    "def load_data_to_table():\n",
    "    source_query = get_config_item('./config/extract_config.yaml', 'source_query')\n",
    "    # create string for primary key matching portion of merge query\n",
    "    merge_condition = get_merge_condition_string(table)\n",
    "    # create string for column matching portion of merge query and all columns string\n",
    "    column_matchings_string = get_column_matchings_string(table)\n",
    "    target_columns_string = get_target_columns_string(table)\n",
    "    source_columns_string = get_source_column_names()\n",
    "    print(\"##### MERGE CONDITION STRING: #####\")\n",
    "    print(merge_condition)\n",
    "    print(\"\\n\")\n",
    "    print(\"##### COLUMN MATCHINGS STRING: #####\")\n",
    "    print(column_matchings_string)\n",
    "    print(\"\\n\")\n",
    "    print(\"##### TARGET COLUMNS STRING: #####\")\n",
    "    print(target_columns_string)\n",
    "    print(\"\\n\")\n",
    "    table_ = table\n",
    "    query = f'''MERGE INTO {catalog}.{index_yaml['schema']}.{table_name} target\n",
    "    USING (\n",
    "        {source_query}) source_table\n",
    "        ON ({merge_condition})\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE\n",
    "            SET {column_matchings_string}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({target_columns_string}) \n",
    "            VALUES ({source_columns_string})\n",
    "        WHEN NOT MATCHED BY SOURCE THEN\n",
    "            DELETE'''.format(catalog=catalog)\n",
    "    print(query)\n",
    "    spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a85015bb-67c0-4071-b1eb-89092d140c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Main function\n",
    "- The main function will invoke the functions declared above.\n",
    "- First the source data is extracted and validated.\n",
    "- Then the target table is created if it doesn't yet exist. If the table exists this step is skipped.\n",
    "- Finally the source date is merged into the target table with records no longer present in the source being deleted from the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eee9114-65bc-4109-899e-2f9a279c5a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Extract\n",
    "    my_table_name_sdf = extract()\n",
    "\n",
    "    # Validate\n",
    "    validate(my_table_name_sdf,\n",
    "            catalog='standardized{}'.format(get_catalog_suffix(target_environment)),\n",
    "            schema=schema,\n",
    "            table=table_name)\n",
    "    full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "    try:\n",
    "        x = spark.sql(f\"SELECT * from {full_table_name}\")\n",
    "    except:\n",
    "        create_initial_table()\n",
    "        add_null_constraints()\n",
    "        add_primary_key_constraints()\n",
    "    load_data_to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27483eac-a18b-45ca-8d3a-e70619e974db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cdc_sql_template",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
