{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a397b07-7cd5-4fae-8152-dad1b1379507",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields, field\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from typing import TypeVar\n",
    "import pyspark.pandas as ps\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bfbe02-f9c2-4af4-ad2f-b924c6e12109",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### AdjudicatedSvcs Fields\n",
    "\n",
    "#### Sourcing the Fields ####\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.adjudicatedsvcs ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsclaims.adjudicatedsvcs* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| cgbeforeadjcoinsuranceamt | Coinsurance Amount:  The 3 Fields (Copay, Deductible, Coinsurance). They are based upon GROSS AMOUNT APPROVED |\n",
    "| cgbeforeadjcopayamt | Copay Amount:  The 3 Fields (Copay, Deductible, Coinsurance). They are based upon GROSS AMOUNT APPROVED |\n",
    "| cgbeforeadjdeductibleamt | Deductible Amount:  The 3 Fields (Copay, Deductible, Coinsurance). They are based upon GROSS AMOUNT APPROVED |\n",
    "| clmnum | Claim Number (MONTHLY SEE FIELD NAME = CLAIM_NO) (MONTHLY SEE FIELD NAME = CLAIM_NO) (Pointer to ^EF0-CLAIMS) |\n",
    "| enctrsvcsvcnum | Encounter Service Service Number (MONTHLY SEE FIELD NAME = ENC_SER_NO) (2 of 2 col; Pointer to ^ENX-ENCOUNTER_SERVICES) |\n",
    "| prof | Professional (MONTHLY SEE FIELD NAME = SERV_PHY_ID) (Pointer to ^DI1D-PHYSICIANS) |\n",
    "| cgbeforeadjamtprepaid | Before Adjustment Amount Prepaid |\n",
    "\n",
    "\n",
    "#### Accessing the Data ####\n",
    "| AdjudicatedSvcs Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| AdjudicatedSvcsData( ).adjudicatedsvcs_base_table( ) | AdjudicatedSvcsData( ).create_adjudicatedsvcs_data_df( ) | AdjudicatedSvcsData( ).view_adjudicatedsvcs_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments ####\n",
    "Summary\n",
    "- AdjudicatedSvcs medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the AdjudicatedSvcsMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed648f2-801a-4880-addb-75d76d7e3a28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalAdjudicatedServices(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def adjudicatedsvcs_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @adjudicatedsvcs_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def adjudicatedsvcs_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_adjudicatedsvcs_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_adjudicatedsvcs_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def adjudicatedsvcs_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AdjudicatedSvcsMetadata:\n",
    "    claim_no: int = field(metadata={\"claim_no\": \"Claim Number (MONTHLY SEE FIELD NAME = CLAIM_NO) (MONTHLY SEE FIELD NAME = CLAIM_NO) (Pointer to ^EF0-CLAIMS)\"})\n",
    "\n",
    "    enc_ser_no: int = field(metadata={\"enc_ser_no\": \"Service line Encounter Serial Number.  Each unique calc_cpt4_cd receives a service line number.\"})\n",
    "\n",
    "    phy_id: int = field(metadata={\"phy_id\": \"Internal physician identification number\"})\n",
    "\n",
    "    pre_paid_amt: int = field(metadata={\"pre_paid_amt\": \"Health plan pre-paid amount after Accounts Payable has processed the claim. Pre_Paid_Amt is sourced directly from CACHE and calculated at the remit level. These uncommon payments are usually for Physical Therapy or Chiropractic care and are generally very small dollar amounts.Source Field = CLAIM_SER_PYMT_IS.PRE_PAID_AMT except for FQHC (Federally Qualified Health Center) lines.  * PURCH_TP_NM = MEDICAID * SERV_CAT_LEVEL1_ID in (1, 3) * CLAIM_FORM_TP_CD = HCFA 1500 * FQHC_FLG = YFQHC Service Year <= 20191. Join to Federal Medicaid file by Service Year, Procedure & Modifier to get FQHC_RVU and FQHC_CONV_FAC.2. When there was not a match, join only by Service Year & Procedure.3. Recalculate PRE_PAID_AMT = UNIT_QTY * FQHC_RVU * FQHC_CONV_FACFQHC Service Year > 20191. Join to Federal Medicaid file by Service Year, Procedure & Modifier to get FQHC_RATE.2. When there was not a match, join only by Service Year & Procedure.3. Recalculate PRE_PAID_AMT = UNIT_QTY * FQHC_RATE* Not allocated to the service line level at extract.  Exactly as it come out of the claim system.\"})\n",
    "    \n",
    "\n",
    "class AdjudicatedSvcsData(MedicalAdjudicatedServices, AdjudicatedSvcsMetadata):\n",
    "    ADJUDICATEDSVCS_DF = spark.read.table('cleansed.adminsystemsclaims.adjudicatedsvcs')\n",
    "    def __init__(self):\n",
    "        self.__adjudicatedsvcs_df = None\n",
    "\n",
    "    @property\n",
    "    def adjudicatedsvcs_dataframe(self):\n",
    "        return self.__adjudicatedsvcs_df\n",
    "    \n",
    "    @adjudicatedsvcs_dataframe.setter\n",
    "    def adjudicatedsvcs_dataframe(self, dataframe):\n",
    "        adjudicatedsvcs_df = dataframe.select(\n",
    "            col(\"CLMNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"claim_no\"].type.__name__))\\\n",
    "                    .alias(\"claim_no\", metadata={\"claim_no\": self.__dataclass_fields__[\"claim_no\"].metadata.get(\"claim_no\")}),\n",
    "            col(\"ENCTRSVCSVCNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"enc_ser_no\"].type.__name__))\\\n",
    "                    .alias(\"enc_ser_no\", metadata={\"enc_ser_no\": self.__dataclass_fields__[\"enc_ser_no\"].metadata.get(\"enc_ser_no\")}),\n",
    "            col(\"PROF\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"phy_id\"].type.__name__))\\\n",
    "                    .alias(\"phy_id\", metadata={\"phy_id\": self.__dataclass_fields__[\"phy_id\"].metadata.get(\"phy_id\")}),\n",
    "            col(\"CGBEFOREADJAMTPREPAID\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"pre_paid_amt\"].type.__name__))\\\n",
    "                    .alias(\"pre_paid_amt\", metadata={\"pre_paid_amt\": self.__dataclass_fields__[\"pre_paid_amt\"].metadata.get(\"pre_paid_amt\")})\n",
    "            )\n",
    "        self.__adjudicatedsvcs_df = adjudicatedsvcs_df\n",
    "\n",
    "    def create_adjudicatedsvcs_data_df(self):\n",
    "        self.adjudicatedsvcs_dataframe = self.ADJUDICATEDSVCS_DF\n",
    "        _adjudicatedsvcs_dataframe = self.adjudicatedsvcs_dataframe\n",
    "        _adjudicatedsvcs_dataframe.display()    \n",
    "        return _adjudicatedsvcs_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_adjudicatedsvcs_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(AdjudicatedSvcsMetadata)\n",
    "            ]\n",
    "        adjudicatedsvcs_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        adjudicatedsvcs_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def adjudicatedsvcs_base_table():\n",
    "        AdjudicatedSvcsData.ADJUDICATEDSVCS_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__adjudicatedsvcs_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17cdce03-6036-46bb-9f89-a8578f0b34f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### BenefitPkg Fields\n",
    "\n",
    "#### Sourcing the Fields ####\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.benefitpkg ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsmember.benefitpkg* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| pkgcode | The Benefit Package Code attached to the members coverage.  Please note package code changes to a Hospice code when member enters Hospice care. |\n",
    "| pkgnum | 'ben_pkg_id' comes from enctrfrppackages.pkgnum |\n",
    "\n",
    "\n",
    "#### Accessing the Data ####\n",
    "| BenefitPkg Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| BenefitPkgData( ).benefitpkg_base_table( ) | BenefitPkgData( ).create_benefitpkg_data_df( ) | BenefitPkgData( ).view_benefitpkg_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments ####\n",
    "Summary\n",
    "- BenefitPkg medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the BenefitPkgMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5de3ad-1a8d-4811-8e94-b5236eb9e5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalBenefitPackage(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def benefitpkg_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @benefitpkg_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def benefitpkg_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_benefitpkg_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_benefitpkg_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def benefitpkg_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BenefitPkgMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    ben_pkg_cd: string = field(metadata={\"ben_pkg_cd\": \"The Benefit Package Code attached to the members coverage.  Please note package code changes to a Hospice code when member enters Hospice care\"})\n",
    "\n",
    "    package_no: int = field(metadata={\"package_no\": \"ben_pkg_id comes from enctrfrppackages.pkgnum\"})\n",
    "    \n",
    "\n",
    "class BenefitPkgData(MedicalBenefitPackage, BenefitPkgMetadata):\n",
    "    BENEFITPKG_DF = spark.read.table('cleansed.adminsystemsmember.benefitpkg')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__benefitpkg_df = None\n",
    "\n",
    "    @property\n",
    "    def benefitpkg_dataframe(self):\n",
    "        return self.__benefitpkg_df\n",
    "    \n",
    "    @benefitpkg_dataframe.setter\n",
    "    def benefitpkg_dataframe(self, dataframe):\n",
    "        benefitpkg_df = dataframe.select(\n",
    "            col(\"PkgCode\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"ben_pkg_cd\"].type.__name__))\\\n",
    "                    .alias(\"ben_pkg_cd\", metadata={\"ben_pkg_cd\": self.__dataclass_fields__[\"ben_pkg_cd\"].metadata.get(\"ben_pkg_cd\")}),\n",
    "            col(\"PkgNum\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"package_no\"].type.__name__))\\\n",
    "                    .alias(\"package_no\", metadata={\"package_no\": self.__dataclass_fields__[\"package_no\"].metadata.get(\"package_no\")})\n",
    "            )\n",
    "        self.__benefitpkg_df = benefitpkg_df\n",
    "\n",
    "    def create_benefitpkg_data_df(self):\n",
    "        self.benefitpkg_dataframe = self.BENEFITPKG_DF\n",
    "        _benefitpkg_dataframe = self.benefitpkg_dataframe\n",
    "        _benefitpkg_dataframe.display()    \n",
    "        return _benefitpkg_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_benefitpkg_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(BenefitPkgMetadata)\n",
    "            ]\n",
    "        benefitpkg_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        benefitpkg_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def benefitpkg_base_table():\n",
    "        BenefitPkgData.BENEFITPKG_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__benefitpkg_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d058c45-93fd-47cb-b2cc-5b0186d82443",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Claims Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.claims ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsclaims.claims* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| acceptancestatus | Acceptance Status (MONTHLY SEE FIELD NAME = ACCEPT_STAT_CD) (Pointer to ^DI3a-ACCEPTANCE_STATUSES) |\n",
    "| amtallowedredbyothins | Amount Allowed Reduction By Other Insurance |\n",
    "| clmnum | Claim Number (MONTHLY SEE FIELD NAME = CLAIM_NO) |\n",
    "| enctrnum | Encounter number generated by the production system. (MONTHLY SEE FIELD NAME = ENC_NO) Pointer to ^ENV-ENCOUNTERS) |\n",
    "| frpnum | FRP Number (MONTHLY SEE FIELD NAME = FRP_NO) |\n",
    "\n",
    "\n",
    "#### Accessing the Data\n",
    "| Claims Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| ClaimsData( ).claims_base_table( ) | ClaimsData( ).create_claims_data_df( ) | ClaimsData( ).view_claims_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- Claims medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the ClaimsMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5a8d69-4132-4911-9dbe-fcb4f4700386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalClaims(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def claims_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @claims_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def claims_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_claims_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_claims_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def claims_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClaimsMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    claim_no: int = field(metadata={\"claim_no\": \"Claim Number (MONTHLY SEE FIELD NAME = CLAIM_NO)\"})\n",
    "\n",
    "    accept_stat_cd: string = field(metadata={\"accept_stat_cd\": \"Indicates whether a claim was accepted or denied, A = ACCEPTED, ~ = MISSING, D = DENIED, P = PENDING\"})\n",
    "\n",
    "    enc_no: int = field(metadata={\"enc_no\": \"The encounter number can be thought of as a billed healthcare event (by a single healthcare-providing entity).\"})\n",
    "\n",
    "    frp_seq_no: int = field(metadata={\"frp_seq_no\": \"FRP_SEQ_NO indicates the order of payer responsibility on an encounter when HealthPartners is both the primary and secondary payer.  This most often happens when a member is covered under HealthPartners as their employer and then has a secondary coverage through their spouse under HealthPartners.\"})\n",
    "\n",
    "\n",
    "class ClaimsData(MedicalClaims, ClaimsMetadata):\n",
    "    CLAIMS_DF = spark.read.table('cleansed.adminsystemsclaims.claims')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__claims_df = None\n",
    "\n",
    "    @property\n",
    "    def claims_dataframe(self):\n",
    "        return self.__claims_df\n",
    "    \n",
    "    @claims_dataframe.setter\n",
    "    def claims_dataframe(self, dataframe):\n",
    "        claims_df = dataframe.select(\n",
    "            col(\"CLMNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"claim_no\"].type.__name__))\\\n",
    "                    .alias(\"claim_no\", metadata={\"claim_no\": self.__dataclass_fields__[\"claim_no\"].metadata.get(\"claim_no\")}), \n",
    "            col(\"ACCEPTANCESTATUS\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"accept_stat_cd\"].type.__name__))\\\n",
    "                    .alias(\"accept_stat_cd\", metadata={\"accept_stat_cd\": self.__dataclass_fields__[\"accept_stat_cd\"].metadata.get(\"accept_stat_cd\")}), \n",
    "            col(\"ENCTRNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"enc_no\"].type.__name__))\\\n",
    "                    .alias(\"enc_no\", metadata={\"enc_no\": self.__dataclass_fields__[\"enc_no\"].metadata.get(\"enc_no\")}),\n",
    "            col(\"FRPNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"frp_seq_no\"].type.__name__))\\\n",
    "                    .alias(\"frp_seq_no\", metadata={\"frp_seq_no\": self.__dataclass_fields__[\"frp_seq_no\"].metadata.get(\"frp_seq_no\")})\n",
    "            )\n",
    "        self.__claims_df = claims_df\n",
    "\n",
    "    def create_claims_data_df(self):\n",
    "        self.claims_dataframe = self.CLAIMS_DF\n",
    "        _claims_dataframe = self.claims_dataframe\n",
    "        _claims_dataframe.display()    \n",
    "        return _claims_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_claims_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(ClaimsMetadata)\n",
    "            ]\n",
    "        medical_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        medical_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def claims_base_table():\n",
    "        ClaimsData.CLAIMS_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__claims_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d01dd0-09e6-4161-8a00-4bd65687c11d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encounters Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.encounters ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsclaims.encounters* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| clmvarclmformtype | Claim Varient Claim Form Type (MONTHLY SEE FIELD NAME = CLAIM_FORM_TP_CD) (Pointer to ^DIW-CLAIM_FORM_TYPES) |\n",
    "| enctrnum | Encounter number generated by the production system. (MONTHLY SEE FIELD NAME = ENC_NO) Pointer to ^ENV-ENCOUNTERS) |\n",
    "| fac | Facility (MONTHLY SEE FIELD NAME = FACIL_ID) (Pointer to ^DI00-FACILITIES) |\n",
    "| firstsvdate | First Service Date (MONTHLY SEE FIELD NAME = FIRST_SERV_DT) |\n",
    "| personnum | Person Number (MONTHLY SEE FIELD NAME = PERSON_NO) (Pointer to ^MB0-PERSONS) |\n",
    "\n",
    "\n",
    "#### Accessing the Data\n",
    "| Encounters Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| EncountersData( ).encounters_base_table( ) | EncountersData( ).create_encounters_data_df( ) | EncountersData( ).view_encounters_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- Encounters medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the EncountersMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c27b7e0-481e-48bf-8605-a0b886e38d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalEncounters(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def encounters_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @encounters_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def encounters_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_encounters_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_encounters_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def encounters_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EncountersMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    claim_form_tp_cd: string = field(metadata={\"claim_form_tp_cd\": \"HCFA 1450 (facility)HCFA 1500 (professional)Narrative Claim (professional)\"})\n",
    "\n",
    "    enc_no: int = field(metadata={\"enc_no\": \"The encounter number can be thought of as a billed healthcare event (by a single healthcare-providing entity).\"})\n",
    "\n",
    "    facil_id: int = field(metadata={\"facil_id\": \"Internal Facility Identification Number.  Identifies the biling entity (facility) of the hospital, clinic or office at which the service was performed. * This is sometimes referred to as HPFIN (HealthPartners Facility Identification Number)To avoid record duplication, descriptive info was obtained from CLAIM_SER_DS.  If duplicates existsed here, the record with the most money in AbsoluteValue(TO_BE_PAID_AMT) and AbsolutValue(MEM_TOT_LIAB_AMT) was kept.If there are still duplicates, keep line with PREVENTIVE_SV_FLG = [blank] before PREVENTIVE_SV_FLG = YAt this point, if there are still duplicates, we let SAS randomly chose which record to keep because we do not know which one is better.\"})\n",
    "\n",
    "    person_no: float = field(metadata={\"person_no\": \"Internal person identification number\"})\n",
    "    \n",
    "\n",
    "class EncountersData(MedicalEncounters, EncountersMetadata):\n",
    "    ENCOUNTERS_DF = spark.read.table('cleansed.adminsystemsclaims.encounters')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__encounters_df = None\n",
    "\n",
    "    @property\n",
    "    def encounters_dataframe(self):\n",
    "        return self.__encounters_df\n",
    "    \n",
    "    @encounters_dataframe.setter\n",
    "    def encounters_dataframe(self, dataframe):\n",
    "        encounters_df = dataframe.select(\n",
    "            col(\"CLMVARCLMFORMTYPE\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"claim_form_tp_cd\"].type.__name__))\\\n",
    "                    .alias(\"claim_form_tp_cd\", metadata={\"claim_form_tp_cd\": self.__dataclass_fields__[\"claim_form_tp_cd\"].metadata.get(\"claim_form_tp_cd\")}), \n",
    "            col(\"ENCTRNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"enc_no\"].type.__name__))\\\n",
    "                    .alias(\"enc_no\", metadata={\"enc_no\": self.__dataclass_fields__[\"enc_no\"].metadata.get(\"enc_no\")}),\n",
    "            col(\"FAC\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"facil_id\"].type.__name__))\\\n",
    "                    .alias(\"facil_id\", metadata={\"facil_id\": self.__dataclass_fields__[\"facil_id\"].metadata.get(\"facil_id\")}),\n",
    "            col(\"PERSONNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"person_no\"].type.__name__))\\\n",
    "                    .alias(\"person_no\", metadata={\"person_no\": self.__dataclass_fields__[\"person_no\"].metadata.get(\"person_no\")}) \n",
    "            )\n",
    "        self.__encounters_df = encounters_df\n",
    "\n",
    "    def create_encounters_data_df(self):\n",
    "        self.encounters_dataframe = self.ENCOUNTERS_DF\n",
    "        _encounters_dataframe = self.encounters_dataframe  \n",
    "        _encounters_dataframe.display()\n",
    "        return _encounters_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_encounters_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(EncountersMetadata)\n",
    "            ]\n",
    "        encounters_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        encounters_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def encounters_base_table():\n",
    "        EncountersData.ENCOUNTERS_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__encounters_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1d615a-dd0a-493b-a7f2-3d27bd801752",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EnctrFrpPackages Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.enctrfrppackages ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsclaims.enctrfrppackages* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| enctrnum | Encounter number generated by the production system. (MONTHLY SEE FIELD NAME = ENC_NO) Pointer to ^ENV-ENCOUNTERS) |\n",
    "| pkgnum | 'ben_pkg_id' comes from enctrfrppackages.pkgnum |\n",
    "| product | Product  (MONTHLY SEE FIELD NAME = PROD_ID) (Pointer to ^DI1Y-PRODUCTS) |\n",
    "| deliverynetwk | Delivery Network (Pointer to ^DIDA(1)-DELIVERY_NETWORK) |2\n",
    "\n",
    " \n",
    "#### Accessing the Data\n",
    "| EnctrFrpPackages Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| EnctrFrpPackagesData( ).enctrfrppackages_base_table( ) | EnctrFrpPackagesData( ).create_enctrfrppackages_data_df( ) | EnctrFrpPackagesData( ).view_enctrfrppackages_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- EnctrFrpPackages medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the EnctrFrpPackagesMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91ab87e-d3d5-469e-9c3c-20dea3ebe4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalEnctrFrpPackages(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def enctrfrppackages_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @enctrfrppackages_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def enctrfrppackages_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_enctrfrppackages_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_enctrfrppackages_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def enctrfrppackages_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EnctrFrpPackagesMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    enc_no: int = field(metadata={\"enc_no\": \"The encounter number can be thought of as a billed healthcare event (by a single healthcare-providing entity).\"})\n",
    "\n",
    "    delivery_network_id: int = field(metadata={\"delivery_network_id\": \"Delivery Network Identifier that indicates the network of care the members coverage has available (i.e. OPEN ACCESS + CIGNA, ALLINA TIERED OPEN ACCESS, etc.)NOTE: To get discrete product groupings, sometimes you need to use PURCH_TP_NM, PURCH_SUB_TP_NM, PROD_NM and DELIVERY_NETWORK_NM.Look in ADW.DELIVERY_NETWORK_DS for dictionary of current values.\"})\n",
    "\n",
    "    package_no: int = field(metadata={\"package_no\": \"ben_pkg_id comes from enctrfrppackages.pkgnum\"})\n",
    "\n",
    "    prod_id: int = field(metadata={\"prod_id\": \"Internal product identifier number. NOTE: To get discrete product groupings, sometimes you need to use PURCH_TP_NM, PURCH_SUB_TP_NM, PROD_NM and DELIVERY_NETWORK_NM.\"})\n",
    "    \n",
    "\n",
    "class EnctrFrpPackagesData(MedicalEnctrFrpPackages, EnctrFrpPackagesMetadata):\n",
    "    ENCTRFRPPACKAGESDATA_DF = spark.read.table('cleansed.adminsystemsclaims.enctrfrppackages')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__enctrfrppackages_df = None\n",
    "\n",
    "    @property\n",
    "    def enctrfrppackages_dataframe(self):\n",
    "        return self.__enctrfrppackages_df\n",
    "    \n",
    "    @enctrfrppackages_dataframe.setter\n",
    "    def enctrfrppackages_dataframe(self, dataframe):\n",
    "        enctrfrppackages_df = dataframe.select(\n",
    "            col(\"ENCTRNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"enc_no\"].type.__name__))\\\n",
    "                    .alias(\"enc_no\", metadata={\"enc_no\": self.__dataclass_fields__[\"enc_no\"].metadata.get(\"enc_no\")}), \n",
    "            col(\"DeliveryNetwk\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"delivery_network_id\"].type.__name__))\\\n",
    "                    .alias(\"delivery_network_id\", metadata={\"delivery_network_id\": self.__dataclass_fields__[\"delivery_network_id\"].metadata.get(\"delivery_network_id\")}), \n",
    "            col(\"PkgNum\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"package_no\"].type.__name__))\\\n",
    "                    .alias(\"package_no\", metadata={\"package_no\": self.__dataclass_fields__[\"package_no\"].metadata.get(\"package_no\")}),\n",
    "            col(\"Product\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"prod_id\"].type.__name__))\\\n",
    "                    .alias(\"prod_id\", metadata={\"prod_id\": self.__dataclass_fields__[\"prod_id\"].metadata.get(\"prod_id\")})\n",
    "            )\n",
    "        self.__enctrfrppackages_df = enctrfrppackages_df\n",
    "\n",
    "    def create_enctrfrppackages_data_df(self):\n",
    "        self.enctrfrppackages_dataframe = self.ENCTRFRPPACKAGESDATA_DF\n",
    "        _enctrfrppackages_dataframe = self.enctrfrppackages_dataframe  \n",
    "        _enctrfrppackages_dataframe.display()\n",
    "        return _enctrfrppackages_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_enctrfrppackages_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(EnctrFrpPackagesMetadata)\n",
    "            ]\n",
    "        enctrfrppackages_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        enctrfrppackages_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def enctrfrppackages_base_table():\n",
    "        EnctrFrpPackagesData.ENCTRFRPPACKAGESDATA_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__enctrfrppackages_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f27ea7d-da3c-4ad5-ba6e-177b0e2ff2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EnctrSvcs Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.enctrsvcs ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsclaims.enctrsvcs* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| cpt4hcpcsgrpcpt4 | CPT4 HCPCS Group CPT4 (Pointer to ^DIg-CPT4S) |\n",
    "| cpt4hcpcsgrpplaceofsvc | CPT4 HCPCS Group Place of Service (MONTHLY SEE FIELD NAME = PLACE_OF_SERV_CD) (Pointer to ^DI1K-PLACES_OF_SERVICE) |\n",
    "| enctrnum | Encounter number generated by the production system. (MONTHLY SEE FIELD NAME = ENC_NO) Pointer to ^ENV-ENCOUNTERS) |\n",
    "\n",
    "\n",
    "#### Accessing the Data\n",
    "| EnctrSvcs Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| EnctrSvcsData( ).enctrsvcs_base_table( ) | EnctrSvcsData( ).create_enctrsvcs_data_df( ) | EnctrSvcsData( ).view_enctrsvcs_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- EnctrSvcs medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the EnctrSvcsMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e10d63-c6ed-49b7-b036-6b328e46eb06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalEnctrSvcs(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def enctrsvcs_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @enctrsvcs_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def enctrsvcs_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_enctrsvcs_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_enctrsvcs_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def enctrsvcs_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EnctrSvcsMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    enc_no: int = field(metadata={\"enc_no\": \"The encounter number can be thought of as a billed healthcare event (by a single healthcare-providing entity).\"})\n",
    "\n",
    "    place_of_serv_cd: string = field(metadata={\"place_of_serv_cd\": \"Type of facility the service was performed atTo avoid record duplication, descriptive info was obtained from CLAIM_SER_DS.  If duplicates existsed here, the record with the most money in AbsoluteValue(TO_BE_PAID_AMT) and AbsolutValue(MEM_TOT_LIAB_AMT) was kept.If there are still duplicates, keep line with PREVENTIVE_SV_FLG = [blank] before PREVENTIVE_SV_FLG = YAt this point, if there are still duplicates, we let SAS randomly chose which record to keep because we do not know which one is better.\"})\n",
    "\n",
    "\n",
    "class EnctrSvcsData(MedicalEnctrSvcs, EnctrSvcsMetadata):\n",
    "    ENCTRSVCS_DF = spark.read.table('cleansed.adminsystemsclaims.enctrsvcs')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__enctrsvcs_df = None\n",
    "\n",
    "    @property\n",
    "    def enctrsvcs_dataframe(self):\n",
    "        return self.__enctrsvcs_df\n",
    "    \n",
    "    @enctrsvcs_dataframe.setter\n",
    "    def enctrsvcs_dataframe(self, dataframe):\n",
    "        enctrsvcs_df = dataframe.select(\n",
    "            col(\"CPT4HCPCSGRPPLACEOFSVC\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"place_of_serv_cd\"].type.__name__))\\\n",
    "                    .alias(\"place_of_serv_cd\", metadata={\"place_of_serv_cd\": self.__dataclass_fields__[\"place_of_serv_cd\"].metadata.get(\"place_of_serv_cd\")}),\n",
    "            col(\"ENCTRNUM\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"enc_no\"].type.__name__))\\\n",
    "                    .alias(\"enc_no\", metadata={\"enc_no\": self.__dataclass_fields__[\"enc_no\"].metadata.get(\"enc_no\")})\n",
    "            )\n",
    "        self.__enctrsvcs_df = enctrsvcs_df\n",
    "\n",
    "    def create_enctrsvcs_data_df(self):\n",
    "        self.enctrsvcs_dataframe = self.ENCTRSVCS_DF\n",
    "        _enctrsvcs_dataframe = self.enctrsvcs_dataframe\n",
    "        _enctrsvcs_dataframe.display()    \n",
    "        return _enctrsvcs_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_enctrsvcs_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(EnctrSvcsMetadata)\n",
    "            ]\n",
    "        enctrsvcs_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        enctrsvcs_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def enctrsvcs_base_table():\n",
    "        EnctrSvcsData.ENCTRSVCS_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__enctrsvcs_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6199b430-cebc-41f0-96ea-a24c859e1683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Person Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.person ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsmember.person* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| personnum | Person Number (MONTHLY SEE FIELD NAME = PERSON_NO) (Pointer to ^MB0-PERSONS) |\n",
    "\n",
    "\n",
    "#### Accessing the Data\n",
    "| Person Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| PersonData( ).person_base_table( ) | PersonData( ).create_person_data_df( ) | PersonData( ).view_person_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- Person medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the PersonMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee53a208-26f5-438d-b6c0-c5853e171c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalPerson(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def person_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @person_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def person_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_person_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_person_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def person_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PersonMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    external_person_id: int = field(metadata={\"external_person_id\": \"External Identification Number.  This is sometimes referred to as member number or chart number.  This is the number you will see on your insurance card.\"})\n",
    "\n",
    "    person_no: float = field(metadata={\"person_no\": \"Internal person identification number\"})\n",
    "\n",
    "\n",
    "class PersonData(MedicalPerson, PersonMetadata):\n",
    "    PERSON_DF = spark.read.table('cleansed.adminsystemsmember.person')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__person_df = None\n",
    "\n",
    "    @property\n",
    "    def person_dataframe(self):\n",
    "        return self.__person_df\n",
    "    \n",
    "    @person_dataframe.setter\n",
    "    def person_dataframe(self, dataframe):\n",
    "        person_df = dataframe.select(\n",
    "            col(\"DemographicsExternalIdNum\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"external_person_id\"].type.__name__))\\\n",
    "                    .alias(\"external_person_id\", metadata={\"external_person_id\": self.__dataclass_fields__[\"external_person_id\"].metadata.get(\"external_person_id\")}),\n",
    "            col(\"PersonNum\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"person_no\"].type.__name__))\\\n",
    "                    .alias(\"person_no\", metadata={\"person_no\": self.__dataclass_fields__[\"person_no\"].metadata.get(\"person_no\")}), \n",
    "            )\n",
    "        self.__person_df = person_df\n",
    "\n",
    "    def create_person_data_df(self):\n",
    "        self.person_dataframe = self.PERSON_DF\n",
    "        _person_dataframe = self.person_dataframe  \n",
    "        _person_dataframe.display()\n",
    "        return _person_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_person_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(PersonMetadata)\n",
    "            ]\n",
    "        person_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        person_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def person_base_table():\n",
    "        PersonData.PERSON_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__person_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5e33de-8f7e-45f2-b006-8ddf113f7a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Products Fields\n",
    "\n",
    "#### Sourcing the Fields\n",
    "Summary\n",
    "- All table fields have been pulled from Adminsystems ( table : cdcods.products ) and have been ingested into the cleansed production cloud environment location: *cleansed.adminsystemsdict.products* \n",
    "\n",
    "\n",
    "| Source Field | Type Comment |\n",
    "| :--- | :--- |\n",
    "| product | Product  (MONTHLY SEE FIELD NAME = PROD_ID) (Pointer to ^DI1Y-PRODUCTS) |\n",
    "| purchasertype | Purchaser Subtype, Purchaser Subtype (1 of 2 col; Pointer to ^MB82-PRODUCT_PURCHASER_SUBTYPES) |\n",
    "| purchasersubtypepurchsubtype | Purchaser Subtype, Purchaser Type (2 of 2 col; Pointer to ^MB82-PRODUCT_PURCHASER_SUBTYPES) |\n",
    "\n",
    "\n",
    "#### Accessing the Data\n",
    "| Products Table | Medical Fields Dataframe | Medical Fields Metadata |\n",
    "| :--- | :--- | :--- |\n",
    "| ProductsData( ).products_base_table( ) | ProductsData( ).create_products_data_df( ) | ProductsData( ).view_products_metadata( ) |\n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- Products medical fields dataframe column types are set using pyspark.sql.Column.**cast**. Assigning the column type within the ProductsMetadata dataclass accomplishes the casting. ( field : type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd971ee-ba6e-4e55-ba9f-37af0e785e33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicalProducts(metaclass = ABCMeta):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def products_dataframe(self):\n",
    "        ...\n",
    "    \n",
    "    @products_dataframe.setter\n",
    "    @abstractmethod\n",
    "    def products_dataframe(self, dataframe):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_products_data_df(self):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def view_products_metadata():\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def products_base_table(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProductsMetadata:\n",
    "    string = TypeVar(\"string\", bound=str)\n",
    "\n",
    "    prod_id: int = field(metadata={\"prod_id\": \"Internal product identifier number. NOTE: To get discrete product groupings, sometimes you need to use PURCH_TP_NM, PURCH_SUB_TP_NM, PROD_NM and DELIVERY_NETWORK_NM.\"})\n",
    "\n",
    "    purch_sub_tp_nm: string = field(metadata={\"purch_sub_tp_nm\": \"Product Purchaser Subtype: Conversion, Cost Group, Cost Individual, Individual, Large Group M+C Group, M+C Individual, PMAP, GAMC, MnCare, Dual Eligible, Select Individual, Small Group, Supplemental IndividualNOTE: To get discrete product groupings, sometimes you need to use PURCH_TP_NM, PURCH_SUB_TP_NM, PROD_NM and DELIVERY_NETWORK_NM.HPUPH Patchif grp_no = 32116 then purch_sub_tp_nm = GROUP - HPUPH; else; Senior Retiree Ntional Choice Patchif prod_id in (848, 855) then PURCH_SUB_TP_NM = GROUP - RNC; else; Note, if people need to find out the original values, they could link back to PROD_DIM by PROD_ID.\"})\n",
    "\n",
    "    purch_tp_nm: string = field(metadata={\"purch_tp_nm\": \"productPurchaser Type full name: Commercial, Medicaid, or Medicare based on product. If SUBSIDIZED INDIVIDUAL EXCHANGE then COMMERCIAL else purch_tp_nmNOTE: To get discrete product groupings, sometimes you need to use PURCH_TP_NM, PURCH_SUB_TP_NM, PROD_NM and DELIVERY_NETWORK_NM.Since members can be enrolled in more than one product we recommend always stratifying analysis by product to prevent double counting.  For example, if a members is enrolled in both Medicare and Commercial products and we administer both plans we will get two bills for the service but the service did not occur twice.\"})\n",
    "\n",
    "    \n",
    "class ProductsData(MedicalProducts, ProductsMetadata):\n",
    "    PRODUCTS_DF = spark.read.table('cleansed.adminsystemsdict.products')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__products_df = None\n",
    "\n",
    "    @property\n",
    "    def products_dataframe(self):\n",
    "        return self.__products_df\n",
    "    \n",
    "    @products_dataframe.setter\n",
    "    def products_dataframe(self, dataframe):\n",
    "        products_df = dataframe.select(\n",
    "            col(\"PurchaserSubtypePurchaserSubtype\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"purch_sub_tp_nm\"].type.__name__))\\\n",
    "                    .alias(\"purch_sub_tp_nm\", metadata={\"purch_sub_tp_nm\": self.__dataclass_fields__[\"purch_sub_tp_nm\"].metadata.get(\"purch_sub_tp_nm\")}), \n",
    "            col(\"PurchaserType\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"purch_tp_nm\"].type.__name__))\\\n",
    "                    .alias(\"purch_tp_nm\", metadata={\"purch_tp_nm\": self.__dataclass_fields__[\"purch_tp_nm\"].metadata.get(\"purch_tp_nm\")}),\n",
    "            col(\"ProductNum\")\\\n",
    "                .cast(str(self.__dataclass_fields__[\"prod_id\"].type.__name__))\\\n",
    "                    .alias(\"prod_id\", metadata={\"prod_id\": self.__dataclass_fields__[\"prod_id\"].metadata.get(\"prod_id\")}), \n",
    "            )\n",
    "        self.__products_df = products_df\n",
    "\n",
    "    def create_products_data_df(self):\n",
    "        self.products_dataframe = self.PRODUCTS_DF\n",
    "        _products_dataframe = self.products_dataframe \n",
    "        _products_dataframe.display()  \n",
    "        return _products_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def view_products_metadata():\n",
    "        metadata = [[\n",
    "            field.name, \n",
    "            field.type.__name__, \n",
    "            field.metadata.get(field.name)] for field in fields(ProductsMetadata)\n",
    "            ]\n",
    "        products_metadata_df = spark.createDataFrame(\n",
    "            metadata, \n",
    "            schema=\"Name STRING, Type STRING, Comments STRING\"\n",
    "            )  \n",
    "        products_metadata_df.display()\n",
    "\n",
    "    @staticmethod\n",
    "    def products_base_table():\n",
    "        ProductsData.PRODUCTS_DF.display()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__products_df.display()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b10fcb-f58a-4e26-bef0-08b2d70d39a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### __Hicore Medical DataFrame\n",
    "\n",
    "\n",
    "#### Dataframe Details\n",
    "| Name | Keys (Primary, Surrogate) | \n",
    "| --- | --- | \n",
    "| claims_df | Surrogate key: 'enc_no' | \n",
    "| encounters_df | Surrogate keys: 'enc_no', 'person_id' | \n",
    "| enctrfrppackages_df | Surrogate keys: 'enc_no', 'prod_id' | \n",
    "| person_df | Surrogate key: 'external_person_id' | \n",
    "| products_df | Surrogate key: 'prod_id' | \n",
    "\n",
    "\n",
    "#### Development Comments\n",
    "Summary\n",
    "- The Hicore Medical dataframe is aggregated when 'HicoreMedicalDataFrame' is instantiated to a new object. Example: new_dataframe = HicoreMedicalDataFrame( ). The complete dataframe is returned with *new_dataframe.hicore_medical_df*. \n",
    "- Hicore Medical table logic occurs in post initialization (__post_init__) of the dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b88d28f-24bf-4f3a-a95d-89a9534a93c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HicoreMedicalDataFrame:\n",
    "    hicore_medical_df: ps.DataFrame = field(default=None)\n",
    "    adjudicatedsvcs_df: ps.DataFrame = field(default=AdjudicatedSvcsData().create_adjudicatedsvcs_data_df(), \\\n",
    "        metadata={\"adjudicatedsvcs_df\": \"Surrogate key: 'claim_no'. Shared relationship with 'Claims' dataframe\"})\n",
    "    benefitpkg_df: ps.DataFrame = field(default=BenefitPkgData().create_benefitpkg_data_df(), \\\n",
    "        metadata={\"benefitpkg_df\": \"Surrogate key: 'ben_pkg_cd'. Shared relationship with 'Enctrfrppackages' dataframe\"})\n",
    "    claims_df: ps.DataFrame = field(default=ClaimsData().create_claims_data_df(), \\\n",
    "        metadata={\"claims_df\": \"Surrogate key: 'enc_no'. Shared relationship with 'Encounters' dataframe\"})\n",
    "    encounters_df: ps.DataFrame = field(default=EncountersData( ).create_encounters_data_df(), \\\n",
    "        metadata={\"ecounters_df\": \"Surrogate key: 'enc_no'. Shared relationship with 'Claims' and 'Enctrfrppackages' dataframes. Surrogate key: 'person_no'. Shared relationship with 'Person' dataframe\"})\n",
    "    enctrsvcs_df: ps.DataFrame = field(default=EnctrSvcsData().create_enctrsvcs_data_df(), \\\n",
    "        metadata={\"claims_df\": \"Surrogate key: 'enc_no'. Shared relationship with 'Enctrfrppackages' dataframe\"})\n",
    "    enctrfrppackages_df: ps.DataFrame = field(default=EnctrFrpPackagesData().create_enctrfrppackages_data_df(), \\\n",
    "        metadata={\"enctrfrppackages_df\": \"Surrogate key: 'enc_no'. Shared relationship with 'Encounters' dataframe. Surrogate key: 'prod_id'. Shared relationship with 'Products' dataframe\"}) \n",
    "    person_df: ps.DataFrame = field(default=PersonData().create_person_data_df(), \\\n",
    "        metadata={\"person_df\": \"Surrogate key: 'external_person_id'. Shared relationship with 'Encounters' dataframe.\"}) \n",
    "    products_df: ps.DataFrame = field(default=ProductsData().create_products_data_df(), \\\n",
    "        metadata={\"products_df\": \"Surrogate key: 'prod_id'. Shared relationship with 'Enctrfrppackages' dataframe.\"}) \n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.hicore_medical_df = self.claims_df \\\n",
    "            .join(self.adjudicatedsvcs_df, \"claim_no\") \\\n",
    "                .join(self.encounters_df, \"enc_no\") \\\n",
    "                    .join(self.enctrfrppackages_df, \"enc_no\") \\\n",
    "                        .join(self.enctrsvcs_df, \"enc_no\") \\\n",
    "                            .join(self.products_df, \"prod_id\") \\\n",
    "                                .join(self.benefitpkg_df, \"package_no\") \\\n",
    "                                    .join(self.person_df, \"person_no\") \\\n",
    "\n",
    "    @property\n",
    "    def transformed_dataframe(self):\n",
    "        return self.hicore_medical_df\n",
    "\n",
    "\n",
    "dataframe = HicoreMedicalDataFrame()\n",
    "hicore_medical_dataframe = dataframe.hicore_medical_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acdeab9-150e-466f-8c52-65ef24b589ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### @Hicore Medical Delta Live Table\n",
    "\n",
    "#### --Materialized View--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e9f7df-742b-48a5-a101-6cc8f10a8359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name='hicore_medical_test',\n",
    "    comment='CLAIM_SER_PYMT_IS and other claims/coverage data in a single location'\n",
    ")\n",
    "def hicore_medical_delta_live():\n",
    "    dataframe = HicoreMedicalDataFrame()\n",
    "    hicore_medical_dataframe = dataframe.hicore_medical_df\n",
    "    return hicore_medical_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f00833-9259-4683-9dd0-483824cc0060",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### @Hicore Medical Table\n",
    "\n",
    "#### --Managed Table--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109aa36a-1dd8-4438-ac52-cd3ad9ee9452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_location = {\n",
    "    'catalog_name': 'standardized_dev',\n",
    "    'schema_name': 'adwhicore',\n",
    "    'table_name': 'hicore_medical_base'\n",
    "}\n",
    "\n",
    "def hicore_medical_table():\n",
    "    dataframe = HicoreMedicalDataFrame()\n",
    "    hicore_medical_dataframe = dataframe.hicore_medical_df\n",
    "    \n",
    "    hicore_medical_dataframe \\\n",
    "        .write.format('delta') \\\n",
    "            .mode('overwrite') \\\n",
    "                .saveAsTable( \\\n",
    "                    f'{table_location[\"catalog_name\"]} \\\n",
    "                        .{table_location[\"schema_name\"]} \\\n",
    "                            .{table_location[\"table_name\"]}')\n",
    "\n",
    "hicore_medical_table()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1764933382657701,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hicore_medical_attribution",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
